# Delaunay Triangulation Optimization Roadmap

## Executive Summary

This document outlines the comprehensive optimization strategy for the Delaunay triangulation library, organized into 4 distinct phases.
The goal is to achieve maximum performance while maintaining 100% backward compatibility for public APIs.

**Overall Status**: Phase 1 âœ… COMPLETE | Phase 2 ðŸ”„ 95% COMPLETE | Phase 3-4 ðŸ“‹ PLANNED

## ðŸŽ¯ Strategic Goals

1. **Eliminate UUIDâ†’Key lookups** in hot paths (80% reduction target)
2. **Reduce memory usage** by 50% through key-based storage
3. **Improve cache locality** via dense data structures
4. **Maintain API stability** throughout all changes
5. **Enable performance tuning** through abstraction layers

---

## ðŸ“Š Phase Overview

| Phase | Name | Status | Impact | Risk |
|-------|------|--------|--------|------|
| 1 | Collection Optimization | âœ… COMPLETE | 2-3x hash performance | Low |
| 2 | Key-Based Internal APIs | ðŸ”„ 95% COMPLETE | 20-40% hot path improvement | Medium |
| 3 | Structure Refactoring | ðŸ“‹ PLANNED | 50% memory reduction | High |
| 4 | Collection Abstraction | ðŸ“‹ PLANNED | 10-15% iteration improvement | Low |

---

## Phase 1: Collection Optimization âœ… COMPLETE

### Status: Completed (September 2025)

### Objective

Replace standard library collections with optimized alternatives throughout the codebase.

### What Was Done

#### Files Modified

- `src/core/algorithms/robust_bowyer_watson.rs`
- `src/core/cell.rs`
- `src/core/traits/insertion_algorithm.rs`
- `src/core/triangulation_data_structure.rs`

#### Changes Made

```rust
// Before: Standard library collections
use std::collections::{HashMap, HashSet};

// After: Optimized FxHash-based collections
use crate::core::collections::{FastHashMap, FastHashSet};
```

### Results

- **2-3x faster** hash operations using FxHasher
- **Better memory locality** for hash-based operations
- **Zero API changes** - internal optimization only
- **All tests passing** with no regressions

### Verification

```bash
âœ… 690/690 tests passing
âœ… No clippy warnings
âœ… Documentation builds successfully
```

---

## Phase 2: Key-Based Internal APIs ðŸ”„ IN PROGRESS

### Status: 95% Complete (December 2025) - FacetCacheProvider pending

### Objective

Eliminate UUIDâ†’Key lookups in hot paths by implementing direct key-based operations.

### What Was Done

#### Core Infrastructure

```rust
// New key-based helper method
fn vertex_keys_for_cell_direct(&self, cell_key: CellKey) 
    -> Result<VertexKeyBuffer, TriangulationValidationError>
```

#### Key-Based Methods Added

```rust
// Cell access
get_cell_by_key() / get_cell_by_key_mut()
contains_cell_key() / cell_keys()
remove_cell_by_key() / remove_cells_by_keys()

// Vertex access
get_vertex_by_key() / get_vertex_by_key_mut()
contains_vertex_key() / vertex_keys()

// Neighbor operations
find_neighbors_by_key()      // #[must_use]
set_neighbors_by_key()       // No panics, proper error handling
find_cells_containing_vertex_by_key()  // #[must_use]
get_cell_vertex_keys()
```

#### Zero-Allocation Iterator Optimization

```rust
// New efficient iterator method
vertex_uuid_iter() -> impl ExactSizeIterator<Item = Uuid>
// Replaces vertex_uuids() -> Vec<Uuid> for iteration patterns
// 1.86x faster, zero heap allocations
```

#### Algorithms Optimized

- `assign_neighbors()` - Uses `vertex_keys_for_cell_direct()`
- `assign_incident_cells()` - Direct key operations
- `remove_duplicate_cells()` - Key-based duplicate detection
- `build_facet_to_cells_hashmap()` - Optimized with keys
- `validate_no_duplicate_cells()` - Key-based validation
- `validate_cell_mappings()` - Direct SlotMap access first

### Results

- **20-40%** reduction in hot path overhead
- **~30%** faster neighbor operations
- **~25%** faster validation operations
- **Eliminated panics** - proper error handling throughout
- **Stack allocation** via SmallVec for D â‰¤ 7
- **Zero-allocation iteration** - Added `vertex_uuid_iter()` eliminating Vec allocations

### Quality Improvements

- Added `#[must_use]` attributes where appropriate
- Added `# Errors` documentation sections
- Eliminated `.unwrap()` calls in favor of proper error handling
- All 690 tests passing with no clippy warnings

### Pending Phase 2 Work: FacetCacheProvider Implementation

- **Status**: ðŸ“‹ TO BE IMPLEMENTED
- **Scope**: Add FacetCacheProvider trait to Bowyer-Watson algorithms
- **Impact**: 50-90% reduction in facet mapping computation time
- **TODO Location**: `src/core/algorithms/robust_bowyer_watson.rs:834-835`
- **Documentation**: Detailed plan in `docs/OPTIMIZING_BOWYER_WATSON.md`

### Implementation Patterns

#### Pattern 1: Key-based internal operations

```rust
// Before: UUID-based internal operations
fn internal_vertex_operation(vertices: &SlotMap<VertexKey, Vertex<...>>, 
                            uuid_map: &UuidToVertexKeyMap, 
                            vertex_uuid: Uuid) {
    let key = uuid_map.get(&vertex_uuid).unwrap();  // Lookup!
    let vertex = &vertices[key];
    // operate on vertex
}

// After: Direct key operations
fn internal_vertex_operation(vertices: &SlotMap<VertexKey, Vertex<...>>, 
                            vertex_key: VertexKey) {
    let vertex = &vertices[vertex_key];
    // operate on vertex - no UUID lookup!
}
```

#### Pattern 2: Public API wrappers

```rust
pub fn public_vertex_operation(&self, vertex_uuid: Uuid) -> Result<...> {
    let vertex_key = self.uuid_to_vertex_key.get(&vertex_uuid)
        .ok_or_else(|| Error::VertexNotFound { uuid: vertex_uuid })?;
    self.internal_vertex_operation(vertex_key)
}
```

#### Migration Examples

```rust
// Using Key-Based Methods
// Instead of:
let cell_key = tds.cell_key_from_uuid(&cell_uuid)?;
let cell = tds.cells().get(cell_key)?;

// Use:
let cell = tds.get_cell_by_key(cell_key)?;

// Working with Neighbors
// Instead of UUID-based operations:
let neighbor_uuids = cell.neighbors.clone();
for neighbor_uuid in neighbor_uuids {
    let neighbor_key = tds.cell_key_from_uuid(&neighbor_uuid)?;
    // ...
}

// Use key-based operations:
let neighbor_keys = tds.find_neighbors_by_key(cell_key);
for neighbor_key in neighbor_keys.into_iter().flatten() {
    // Direct key usage, no lookup needed
}
```

#### Anti-pattern eliminated

```rust
// Anti-pattern: Internal algorithms doing UUID lookups
fn process_cell_neighbors(tds: &Tds<...>, cell_uuid: Uuid) {
    let cell_key = tds.uuid_to_cell_key.get(&cell_uuid).unwrap(); // Lookup!
    let cell = &tds.cells[cell_key];
    
    if let Some(neighbor_uuids) = &cell.neighbors {
        for neighbor_uuid in neighbor_uuids.iter().flatten() {
            let neighbor_key = tds.uuid_to_cell_key.get(neighbor_uuid).unwrap(); // More lookups!
        }
    }
}

// Fixed: TDS provides key-based operations
fn process_cell_neighbors_by_key(&self, cell_key: CellKey) {
    let cell = &self.cells[cell_key]; // Direct access!
    // Use find_neighbors_by_key() for key-based neighbor access
    let neighbor_keys = self.find_neighbors_by_key(cell_key);
    for neighbor_key in neighbor_keys.into_iter().flatten() {
        // Direct key operations
    }
}
```

---

## Phase 3: Structure Refactoring ðŸ“‹ PLANNED

### Status: Not Started

### Target: Q1 2026

### Objective

Refactor Cell, Vertex, and Facet structures to store keys directly instead of full objects.

### Planned Changes

#### Cell Structure

```rust
// Current: Stores full Vertex objects
pub struct Cell<T, U, V, const D: usize> {
    uuid: Uuid,
    vertices: Vec<Vertex<T, U, D>>,        // Full objects
    neighbors: Option<Vec<Option<Uuid>>>,   // UUIDs
    data: V,
}

// Target: Store only keys
pub struct Cell<T, U, V, const D: usize> {
    uuid: Uuid,
    vertex_keys: [VertexKey; D+1],         // Just keys
    neighbor_keys: Option<Vec<Option<CellKey>>>,  // Keys
    data: V,
}
```

#### Vertex Structure

```rust
// Current
pub struct Vertex<T, U, const D: usize> {
    uuid: Uuid,
    point: Point<T, D>,
    incident_cell: Option<Uuid>,  // UUID
    data: U,
}

// Target
pub struct Vertex<T, U, const D: usize> {
    uuid: Uuid,
    point: Point<T, D>,
    incident_cell: Option<CellKey>,  // Key
    data: U,
}
```

#### Facet Structure (Complete Redesign)

```rust
// Current: Heavyweight with full objects (18x larger than needed!)
pub struct Facet<T, U, V, const D: usize> {
    cell: Cell<T, U, V, D>,      // Full cell object
    vertex: Vertex<T, U, D>,     // Full vertex object
}

// Target: Lightweight view into TDS
pub struct Facet<'tds, T, U, V, const D: usize> {
    tds: &'tds Tds<T, U, V, D>,
    cell_key: CellKey,
    facet_index: u8,  // Which facet of the cell (0..D)
}

impl<'tds, T, U, V, const D: usize> Facet<'tds, T, U, V, D> {
    // All data comes from TDS - zero duplication
    fn vertices(&self) -> impl Iterator<Item = &'tds Vertex<T, U, D>> {
        let cell = &self.tds.cells[self.cell_key];
        cell.vertices()
            .iter() 
            .enumerate()
            .filter(|(i, _)| *i != self.facet_index as usize)
            .map(|(_, vertex)| vertex)
    }
    
    fn opposite_vertex(&self) -> &'tds Vertex<T, U, D> {
        let cell = &self.tds.cells[self.cell_key];
        &cell.vertices()[self.facet_index as usize]
    }
}
```

#### ConvexHull Storage Strategy

```rust
// Option 1: Store facet descriptors
struct ConvexHull<T, U, V, const D: usize> {
    tds: Arc<Tds<T, U, V, D>>,
    boundary_facets: Vec<(CellKey, u8)>,  // Minimal storage
}

// Option 2: Compute on-demand (preferred)
impl ConvexHull {
    fn facets(&self) -> impl Iterator<Item = Facet<'_, T, U, V, D>> {
        self.tds.boundary_facets()
            .map(|info| Facet {
                tds: &self.tds,
                cell_key: info.cell_key,
                facet_index: info.facet_index,
            })
    }
}
```

### Expected Impact

- **50% memory reduction** for Cell structures
- **18x memory reduction** for Facet structures (from full objects to lightweight views)
- **Complete elimination** of remaining UUID lookups
- **Better cache locality** with smaller structures
- **Simpler serialization** with POD types
- **Prevention of stale data** - Facets as views ensure consistency

### Migration Strategy

1. Create new structures (`CellV2`, etc.) alongside existing
2. Implement conversion methods between old/new formats
3. Migrate algorithms to use new structures
4. Deprecate old structures
5. Remove in next major version

### Risks

- **High API impact** - breaking changes for some users
- **Serialization compatibility** - need migration path
- **Complex refactoring** - touches core data structures

---

## Phase 4: Collection Abstraction ðŸ“‹ PLANNED

### Status: Not Started

### Target: Q2 2026

### Objective

Abstract SlotMap usage behind traits to enable swapping implementations without code changes.

### Design

#### Core Trait

```rust
pub trait StableKeyCollection<K: Key, V> {
    fn insert(&mut self, value: V) -> K;
    fn remove(&mut self, key: K) -> Option<V>;
    fn get(&self, key: K) -> Option<&V>;
    fn get_mut(&mut self, key: K) -> Option<&mut V>;
    fn contains_key(&self, key: K) -> bool;
    fn len(&self) -> usize;
    fn keys(&self) -> impl Iterator<Item = K> + '_;
    fn values(&self) -> impl Iterator<Item = &V> + '_;
    fn iter(&self) -> impl Iterator<Item = (K, &V)> + '_;
    fn clear(&mut self);
}
```

#### TDS Becomes Generic

```rust
pub struct Tds<
    T, U, V, const D: usize,
    CC = SlotMapCollection<CellKey, Cell<T, U, V, D>>,
    VC = SlotMapCollection<VertexKey, Vertex<T, U, D>>,
> where
    CC: StableKeyCollection<CellKey, Cell<T, U, V, D>>,
    VC: StableKeyCollection<VertexKey, Vertex<T, U, D>>,
{
    pub cells: CC,
    pub vertices: VC,
    // ...
}
```

#### Implementations

- `SlotMapCollection` - Current default
- `DenseSlotMapCollection` - Better cache locality
- `HopSlotMapCollection` - For very large triangulations
- Custom implementations for specific use cases

### Expected Impact

#### Performance Characteristics Comparison

| Aspect | SlotMap (Current) | DenseSlotMap | HopSlotMap |
|--------|------------------|--------------|------------|
| Insert | O(1) amortized | O(1) amortized | O(1) |
| Remove | O(1) | O(1) with moves | O(1) |
| Lookup | O(1) | O(1) | O(1) |
| Memory | Sparse | Dense/contiguous | Hop-optimized |
| Iteration | Good | **Excellent** | Good |
| Best For | Dynamic changes | Stable/iteration | Large scale |

#### Expected Improvements

- **10-15% improvement** in iteration-heavy operations with DenseSlotMap
- **5-10% better** cache miss rate
- **Zero code changes** required to swap implementations
- **Enable benchmarking** of different strategies

### Implementation Strategy

#### Phase 4A: Trait Definition and SlotMap Wrapper

1. Define `StableKeyCollection` trait
2. Implement `SlotMapCollection` wrapper
3. Update TDS to use trait bounds
4. Ensure all tests pass with default implementation

#### Phase 4B: Alternative Implementations  

1. Implement `DenseSlotMapCollection`
2. Add benchmarks comparing implementations
3. Document performance characteristics
4. Consider `HopSlotMap` for very large triangulations

#### Phase 4C: Specialized Optimizations

1. Custom allocator support
2. Memory pool implementations for specific use cases
3. SIMD-friendly layouts (if applicable)

### Benefits

- **Performance tuning** without code changes
- **Future flexibility** for custom implementations
- **Clean abstraction** - single interface for all
- **Easy benchmarking** - swap with type parameter

---

## ðŸ”„ Migration Path for Users

### Phase 1-2: No Changes Required âœ…

- Internal optimizations only
- 100% API compatibility maintained
- Automatic performance improvements

### Phase 3: Deprecation Warnings

```rust
#[deprecated(since = "0.6.0", note = "Use CellV2 for better performance")]
pub struct Cell<T, U, V, const D: usize> { /* ... */ }
```

### Phase 4: Opt-in Performance

```rust
// Default usage unchanged
let tds: Tds<f64, (), (), 3> = Tds::new(&vertices)?;

// Advanced users can optimize
use delaunay::collections::DenseSlotMapCollection;
type OptimizedTds<T, U, V, const D: usize> = 
    Tds<T, U, V, D, DenseSlotMapCollection<_>, DenseSlotMapCollection<_>>;
```

---

## ðŸ“ˆ Performance Targets

### Overall Goals

- **80% reduction** in UUIDâ†’Key lookups âœ… (Phase 2)
- **50% memory reduction** for Cell structures (Phase 3)
- **40% improvement** in hot path operations âœ… (Phase 1-2)
- **15% improvement** in iteration performance (Phase 4)

### Current Achievement (Phase 1-2)

- âœ… 2-3x faster hash operations
- âœ… 20-40% hot path improvement
- âœ… 25-30% faster validation
- âœ… Zero API breakage

---

## ðŸš€ Success Metrics

### Code Quality

- [x] All tests passing (690/690) âœ…
- [x] No clippy warnings âœ…
- [x] Documentation complete âœ…
- [ ] Phase 3 migration guide written
- [ ] Phase 4 benchmarks implemented

### Performance

- [x] Hash operations optimized âœ…
- [x] Key-based operations implemented âœ…
- [ ] Memory usage reduced by 50%
- [ ] Collection abstraction benchmarked

### Compatibility

- [x] Phase 1-2: 100% backward compatible âœ…
- [ ] Phase 3: Migration path documented
- [ ] Phase 4: Type aliases for compatibility

---

## ðŸ“š Related Documentation

### Core Optimization Documents

- **This document (`OPTIMIZATION_ROADMAP.md`)** - Comprehensive 4-phase optimization roadmap (primary reference)
- `docs/optimization_recommendations.md` - Historical optimization analysis with detailed code examples
  - Contains original implementation details and code samples
  - Useful for understanding the evolution of optimizations
  - Most recommendations have been implemented or incorporated into this roadmap

### Completed Micro-Optimizations

- `docs/vertex_uuid_iter_optimizations.md` - Zero-allocation UUID iteration (Phase 2)
  - **Status**: âœ… COMPLETE
  - **Impact**: 1.86x faster iteration, zero heap allocations
  - **Scope**: Iterator pattern for Cell vertex UUIDs

### Phase 2 Implementation Guides

- `docs/OPTIMIZING_BOWYER_WATSON.md` - FacetCacheProvider implementation for Bowyer-Watson algorithms
  - **Part of Phase 2** - Pending implementation
  - **Impact**: 50-90% reduction in facet mapping computation time
  - **Rationale**: Eliminates redundant computation in hot paths

---

## ðŸ“œ Historical Context

### Original Optimization Project Achievements

Before the 4-phase optimization roadmap, the Pure Incremental Delaunay Triangulation refactoring project achieved:

- **Buffer reuse system**: InsertionBuffers with reusable collections
- **Optimized validation**: Early termination and pre-computed maps
- **Pure incremental algorithm**: Eliminated supercell complexity
- **Robust geometric predicates**: Enhanced numerical stability
- **Multi-strategy insertion**: Cavity-based and hull extension
- **Memory profiling**: Allocation tracking with count-allocations feature

These achievements laid the foundation for the current 4-phase optimization roadmap.

### Orthogonal Improvements

**Numerical Robustness**: Separately from performance optimization, the library includes comprehensive numerical stability improvements
documented in [`numerical_robustness_guide.md`](./numerical_robustness_guide.md). These address geometric predicate stability and
the "No cavity boundary facets found" error through robust predicates and fallback strategies.

---

## ðŸŽ‰ Conclusion

Phase 1 is fully complete and Phase 2 is 95% complete (pending FacetCacheProvider implementation), together providing significant
performance improvements while maintaining full backward compatibility. The remaining Phase 2 work and Phases 3-4 are well-designed
and ready for implementation, with clear goals and migration strategies.

The optimization roadmap balances performance gains with API stability, ensuring users benefit from improvements without disruption.
