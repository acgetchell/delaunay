name: Performance Regression Testing

# Run performance regression testing on important changes
# Security: All GitHub context variables are passed through safe environment variables
# to prevent code injection attacks via malicious branch names or commit data
on:
  # Manual trigger
  workflow_dispatch:

  # Pull requests to main branch
  pull_request:
    branches:
      - main
    # Only run on changes that could affect performance
    paths:
      - 'src/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'

  # On pushes to main branch
  push:
    branches:
      - main
    # Only run on changes that could affect performance
    paths:
      - 'src/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'

# Security: Define minimal required permissions
permissions:
  contents: read
  actions: read
  pull-requests: read

concurrency:
  group: perf-regress-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  performance-regression:
    runs-on: macos-15
    timeout-minutes: 45

    steps:
      - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8  # v5.0.0
        with:
          fetch-depth: 0  # required to diff against BASELINE_COMMIT

      - name: Install Rust toolchain
        uses: actions-rust-lang/setup-rust-toolchain@ac90e63697ac2784f4ecfe2964e1a285c304003a  # v1.14.1
        with:
          cache: true
          # Toolchain from rust-toolchain.toml; no extra target needed on macOS-ARM

      - name: Install uv (Python package manager)
        uses: astral-sh/setup-uv@557e51de59eb14aaaba2ed9621916900a91d50c6  # v6.6.1
        with:
          version: "latest"

      - name: Verify uv installation
        run: uv --version


      - name: Find baseline artifact
        id: find_baseline
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea  # v7.0.1
        with:
          script: |
            try {
              // Precompute expected baseline names for early-hit optimization
              const releases = await github.paginate(
                github.rest.repos.listReleases,
                { owner: context.repo.owner, repo: context.repo.repo, per_page: 50 }
              );

              const expectedReleaseBaselines = new Set();
              for (const release of releases) {
                if (!release.draft && !release.prerelease) {
                  const cleanTag = release.tag_name.replace(/[^a-zA-Z0-9._-]/g, '_');
                  expectedReleaseBaselines.add(`performance-baseline-${cleanTag}`);
                }
              }
              console.log(`Precomputed ${expectedReleaseBaselines.size} expected release baseline names`);

              // Fetch all successful generate-baseline.yml runs once (O(runs) API calls)
              console.log('Fetching recent generate-baseline.yml runs...');
              let count = 0;
              const runs = await github.paginate(
                github.rest.actions.listWorkflowRuns,
                {
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: 'generate-baseline.yml',
                  status: 'completed',
                  conclusion: 'success',
                  per_page: 100
                },
                (response, done) => {
                  // Limit to 150 runs total across pages (no overshoot)
                  const remaining = Math.max(0, 150 - count);
                  if (remaining === 0) { done(); return []; }
                  const slice = response.data.slice(0, remaining);
                  count += slice.length;
                  if (count >= 150) done();
                  return slice;
                }
              );

              console.log(`Found ${runs.length} successful generate-baseline runs`);

              // Build artifact cache: artifact name → {run_id, run_created_at}
              const artifactCache = new Map();
              let foundReleaseBaseline = false;
              for (const run of runs) {
                try {
                  const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    run_id: run.id
                  });

                  for (const artifact of artifacts.data.artifacts) {
                    if (artifact.name.startsWith('performance-baseline-') && artifact.expired !== true) {
                      if (!artifactCache.has(artifact.name)) {
                        artifactCache.set(artifact.name, {
                          run_id: run.id,
                          run_created_at: run.created_at
                        });

                        // Early-hit optimization: stop if we found a release baseline
                        if (expectedReleaseBaselines.has(artifact.name)) {
                          console.log(`Early hit: found release baseline ${artifact.name}, stopping search`);
                          foundReleaseBaseline = true;
                        }
                      }
                    }
                  }

                  // Short-circuit if we found a release baseline
                  if (foundReleaseBaseline) break;
                } catch (error) {
                  console.log(`Warning: Could not fetch artifacts for run ${run.id}: ${error.message}`);
                  continue;
                }
              }

              console.log(`Built cache of ${artifactCache.size} baseline artifacts`);

              console.log(`Found ${releases.length} releases (already fetched)`);

              // Look for releases with baseline artifacts (now O(releases) lookups)
              for (const release of releases) {
                console.log(`Checking release ${release.tag_name}...`);
                if (release.draft || release.prerelease) {
                  console.log(`Skipping draft/prerelease ${release.tag_name}`);
                  continue;
                }

                // Must match sanitize step in .github/workflows/generate-baseline.yml
                const cleanTag = release.tag_name.replace(/[^a-zA-Z0-9._-]/g, '_');
                const expectedName = `performance-baseline-${cleanTag}`;

                if (artifactCache.has(expectedName)) {
                  const artifactInfo = artifactCache.get(expectedName);
                  console.log(
                    `Found release baseline artifact ${expectedName} in run ${artifactInfo.run_id} ` +
                    `for release ${release.tag_name}`
                  );
                  core.setOutput('found', 'true');
                  core.setOutput('release_tag', release.tag_name);
                  core.setOutput('artifact_name', expectedName);
                  core.setOutput('run_id', artifactInfo.run_id.toString());
                  core.setOutput('source_type', 'release');
                  return;
                }
              }

              console.log('No release baseline artifacts found, checking for any recent baselines...');

              // Fallback: look for any recent baseline artifact (including manual runs)
              if (artifactCache.size > 0) {
                // Find the most recent artifact (by run creation time)
                let mostRecentArtifact = null;
                let mostRecentTime = null;

                for (const [artifactName, artifactInfo] of artifactCache.entries()) {
                  const runTime = new Date(artifactInfo.run_created_at);
                  if (!mostRecentTime || runTime > mostRecentTime) {
                    mostRecentTime = runTime;
                    mostRecentArtifact = { name: artifactName, ...artifactInfo };
                  }
                }

                if (mostRecentArtifact) {
                  console.log(
                    `Found baseline artifact ${mostRecentArtifact.name} in run ${mostRecentArtifact.run_id} ` +
                    `(created: ${mostRecentArtifact.run_created_at})`
                  );
                  core.setOutput('found', 'true');
                  core.setOutput('release_tag', 'manual-baseline');
                  core.setOutput('artifact_name', mostRecentArtifact.name);
                  core.setOutput('run_id', mostRecentArtifact.run_id.toString());
                  core.setOutput('source_type', 'manual');
                  return;
                }
              }

              console.log('No baseline artifacts found in any recent runs');
              core.setOutput('found', 'false');
            } catch (error) {
              console.error(`Error searching for baseline artifacts: ${error.message}`);
              core.setOutput('found', 'false');
            }

      - name: Download latest baseline artifact
        if: steps.find_baseline.outputs.found == 'true'
        uses: actions/download-artifact@634f93cb2916e3fdff6788551b99b062d0335ce0  # v5.0.0
        continue-on-error: true
        with:
          name: ${{ steps.find_baseline.outputs.artifact_name }}
          path: baseline-artifact/
          run-id: ${{ steps.find_baseline.outputs.run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Prepare baseline for comparison
        if: steps.find_baseline.outputs.found == 'true'
        shell: bash
        env:
          RELEASE_TAG: ${{ steps.find_baseline.outputs.release_tag }}
          SOURCE_TYPE: ${{ steps.find_baseline.outputs.source_type }}
        run: |
          set -euo pipefail
          if [[ -f "baseline-artifact/baseline_results.txt" ]]; then
            echo "📦 Prepared baseline from artifact (origin: ${SOURCE_TYPE:-unknown}, tag: ${RELEASE_TAG:-n/a})"
            echo "BASELINE_EXISTS=true" >> "$GITHUB_ENV"
            echo "BASELINE_SOURCE=artifact" >> "$GITHUB_ENV"
            echo "BASELINE_ORIGIN=${SOURCE_TYPE:-unknown}" >> "$GITHUB_ENV"
            if [[ -n "${RELEASE_TAG:-}" ]]; then
              echo "BASELINE_TAG=${RELEASE_TAG}" >> "$GITHUB_ENV"
            fi

            # Show baseline metadata
            echo "=== Baseline Information (from artifact) ==="
            head -n 3 baseline-artifact/baseline_results.txt
          else
            echo "❌ Downloaded artifact but no baseline_results.txt found"
            echo "BASELINE_EXISTS=false" >> "$GITHUB_ENV"
            echo "BASELINE_SOURCE=missing" >> "$GITHUB_ENV"
          fi

      - name: Set baseline status if none found
        if: steps.find_baseline.outputs.found != 'true'
        shell: bash
        run: |
          set -euo pipefail
          echo "📈 No baseline artifact found for performance comparison"
          echo "BASELINE_EXISTS=false" >> "$GITHUB_ENV"
          echo "BASELINE_SOURCE=none" >> "$GITHUB_ENV"
          echo "BASELINE_ORIGIN=none" >> "$GITHUB_ENV"

      - name: Extract baseline commit SHA
        if: env.BASELINE_EXISTS == 'true'
        shell: bash
        run: |
          set -euo pipefail
          bc_sha="$(grep "^Git commit:" baseline-artifact/baseline_results.txt | awk '{print $3}' || true)"
          if [[ -z "$bc_sha" || ! "$bc_sha" =~ ^[0-9A-Fa-f]{7,40}$ ]]; then
            if [[ -f "baseline-artifact/metadata.json" ]]; then
              bc_sha="$(python3 -c 'import json,sys; p="baseline-artifact/metadata.json"; \
d=json.load(open(p)); print(d.get("commit",""))' || true)"
            fi
          fi
          if [[ -n "$bc_sha" && "$bc_sha" =~ ^[0-9A-Fa-f]{7,40}$ ]]; then
            echo "BASELINE_COMMIT=$bc_sha" >> "$GITHUB_ENV"
          else
            echo "BASELINE_COMMIT=unknown" >> "$GITHUB_ENV"
          fi

      - name: Determine if benchmarks should run
        if: env.BASELINE_EXISTS == 'true'
        shell: bash
        env:
          SAFE_COMMIT_SHA: ${{ github.sha }}
        run: |
          set -euo pipefail
          if [[ "$BASELINE_COMMIT" == "unknown" ]]; then
            echo "SKIP_BENCHMARKS=false" >> "$GITHUB_ENV"
            echo "SKIP_REASON=unknown_baseline" >> "$GITHUB_ENV"
          elif [[ "$BASELINE_COMMIT" == "$SAFE_COMMIT_SHA" ]]; then
            echo "SKIP_BENCHMARKS=true" >> "$GITHUB_ENV"
            echo "SKIP_REASON=same_commit" >> "$GITHUB_ENV"
          else
            if ! git cat-file -e "$BASELINE_COMMIT^{commit}" 2>/dev/null; then
              echo "SKIP_BENCHMARKS=false" >> "$GITHUB_ENV"
              echo "SKIP_REASON=baseline_commit_not_found" >> "$GITHUB_ENV"
            elif git diff --name-only "$BASELINE_COMMIT"..HEAD | \
                   grep -qE '^(src/|benches/|Cargo\.toml|Cargo\.lock)'; then
              echo "SKIP_BENCHMARKS=false" >> "$GITHUB_ENV"
              echo "SKIP_REASON=changes_detected" >> "$GITHUB_ENV"
            else
              echo "SKIP_BENCHMARKS=true" >> "$GITHUB_ENV"
              echo "SKIP_REASON=no_relevant_changes" >> "$GITHUB_ENV"
            fi
          fi

      - name: Skip benchmarks - no code changes
        if: env.BASELINE_EXISTS == 'true' && env.SKIP_BENCHMARKS == 'true'
        shell: bash
        run: |
          set -euo pipefail
          case "${SKIP_REASON:-unknown}" in
            same_commit) echo "🔍 Current commit matches baseline (${BASELINE_COMMIT}); skipping benchmarks.";;
            no_relevant_changes) echo "🔍 No relevant code changes since ${BASELINE_COMMIT}; skipping benchmarks.";;
            *) echo "🔍 Benchmarks skipped.";;
          esac

      - name: Skip benchmarks - no baseline available
        if: env.BASELINE_EXISTS != 'true'
        shell: bash
        run: |
          set -euo pipefail
          echo "⚠️ No performance baseline available for comparison."
          echo "   - No baseline artifacts found in recent workflow runs"
          echo "   - Performance regression testing requires a baseline"
          echo ""
          echo "💡 To enable performance regression testing:"
          echo "   1. Create a release tag (e.g., v0.4.3), or"
          echo "   2. Manually trigger the 'Generate Performance Baseline' workflow"
          echo "   3. Future PRs and pushes will use that baseline for comparison"
          echo "   4. Baselines use full benchmark settings for accurate comparisons"

      - name: Run performance regression test
        if: env.BASELINE_EXISTS == 'true' && env.SKIP_BENCHMARKS == 'false'
        shell: bash
        run: |
          set -euo pipefail
          echo "🚀 Running performance regression test..."
          echo "   Baseline source: ${BASELINE_SOURCE:-unknown}"
          echo "   Baseline origin: ${BASELINE_ORIGIN:-unknown}"

          # This will exit with code 1 if significant regressions are found
          echo "   Using full comparison mode against ${BASELINE_ORIGIN:-unknown} baseline"
          if uv run benchmark-utils --help >/dev/null 2>&1; then
            uv run benchmark-utils compare --baseline baseline-artifact/baseline_results.txt
          elif uv run python -c "import importlib; importlib.import_module('scripts.benchmark_utils')" \
               >/dev/null 2>&1; then
            uv run python -m scripts.benchmark_utils compare --baseline baseline-artifact/baseline_results.txt
          else
            echo "❌ benchmark-utils entrypoint and module not found" >&2
            exit 2
          fi

      - name: Display regression test results
        if: env.BASELINE_EXISTS == 'true' && env.SKIP_BENCHMARKS == 'false' && always()
        shell: bash
        run: |
          set -euo pipefail
          if [[ -f "benches/compare_results.txt" ]]; then
            echo "=== Performance Regression Test Results ==="
            cat benches/compare_results.txt
          else
            echo "⚠️ No comparison results file found"
          fi

      - name: Upload regression test results
        if: env.BASELINE_EXISTS == 'true' && env.SKIP_BENCHMARKS == 'false' && always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: performance-regression-results-${{ github.run_number }}
          path: benches/compare_results.txt
          if-no-files-found: warn
          retention-days: 30

      - name: Summary
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          echo "📊 Performance Regression Testing Summary"
          echo "==========================================="
          echo "Baseline source: ${BASELINE_SOURCE:-none}"
          echo "Baseline origin: ${BASELINE_ORIGIN:-unknown}"
          echo "Baseline tag: ${BASELINE_TAG:-n/a}"
          echo "Baseline exists: ${BASELINE_EXISTS:-false}"
          echo "Skip benchmarks: ${SKIP_BENCHMARKS:-unknown}"
          echo "Skip reason: ${SKIP_REASON:-n/a}"

          if [[ "${BASELINE_EXISTS:-false}" == "true" && "${SKIP_BENCHMARKS:-true}" == "false" ]]; then
            if [[ -f "benches/compare_results.txt" ]]; then
              if grep -q "REGRESSION" benches/compare_results.txt; then
                echo "Result: ⚠️ Performance regressions detected"
              else
                echo "Result: ✅ No significant performance regressions"
              fi
            else
              echo "Result: ❓ Benchmark comparison completed but no results file found"
            fi
          elif [[ "${SKIP_BENCHMARKS:-true}" == "true" ]]; then
            case "${SKIP_REASON:-unknown}" in
              same_commit) echo "Result: ⏭️ Benchmarks skipped (same commit as baseline)";;
              no_relevant_changes) echo "Result: ⏭️ Benchmarks skipped (no relevant code changes)";;
              baseline_commit_not_found) \
                echo "Result: ⚠️ Baseline commit not found in history (force-push/shallow clone?)";;
              *) echo "Result: ⏭️ Benchmarks skipped";;
            esac
          else
            echo "Result: ⏭️ Benchmarks skipped (no baseline available)"
          fi
