# Delaunay Library Performance Results

This file contains performance benchmarks and analysis for the delaunay library.
The results are automatically generated and updated by the benchmark infrastructure.

**Last Updated**: 2025-10-16 21:04:24 UTC
**Generated By**: benchmark_utils.py
**Git Commit**: 8a09e6020ec0dcf55a2cc146d8ddb05a48aee1a2
**Hardware**: Apple M4 Max (16 cores)
**Memory**: 64.0 GB
**OS**: macOS
**Rust**: rustc 1.90.0 (1159e78c4 2025-09-14)

## Performance Results Summary

### Circumsphere Performance Results

#### Version 0.5.1 Results (2025-10-16)

#### Single Query Performance (2D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 2D | 583 ns | 591 ns | 460 ns | **insphere_lifted** |
| Boundary vertex | 613 ns | 590 ns | 462 ns | **insphere_lifted** |
| Far vertex | 618 ns | 582 ns | 466 ns | **insphere_lifted** |

#### Single Query Performance (3D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 3D | 911 ns | 1.4 µs | 702 ns | **insphere_lifted** |
| Boundary vertex | 911 ns | 1.4 µs | 714 ns | **insphere_lifted** |
| Far vertex | 890 ns | 1.4 µs | 699 ns | **insphere_lifted** |

#### Single Query Performance (4D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 4D | 1.2 µs | 1.8 µs | 1.1 µs | **insphere_lifted** |
| Boundary vertex | 1.3 µs | 1.8 µs | 1.0 µs | **insphere_lifted** |
| Far vertex | 1.3 µs | 1.8 µs | 1.0 µs | **insphere_lifted** |

#### Single Query Performance (5D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 5D | 1.7 µs | 2.9 µs | 1.4 µs | **insphere_lifted** |
| Boundary vertex | 1.7 µs | 2.9 µs | 1.5 µs | **insphere_lifted** |
| Far vertex | 1.8 µs | 2.9 µs | 1.5 µs | **insphere_lifted** |

## Key Findings

### Performance Ranking

1. **insphere_lifted** - (fastest) - Consistently best performance across all tests
2. **insphere** - (middle) - ~1.2x slower than fastest
3. **insphere_distance** - (slowest) - ~1.8x slower than fastest

### Numerical Accuracy Analysis

Based on random test cases:

- **insphere vs insphere_distance**: 100.0% agreement
- **insphere vs insphere_lifted**: 100.0% agreement (different algorithms)
- **insphere_distance vs insphere_lifted**: 100.0% agreement
- **All three methods agree**: 100.0% (expected due to different numerical approaches)

## Recommendations

### For Performance-Critical Applications

- **Use `insphere_lifted`** for maximum performance
- ~1.8x faster than `insphere_distance`
- Best choice for batch processing and high-frequency queries
- Recommended for applications requiring millions of containment tests

### For Numerical Stability

- **Use `insphere`** for most reliable results
- Standard determinant-based approach with proven mathematical properties
- Good balance of performance and numerical stability
- Recommended for applications where correctness is paramount

### For Educational/Research Purposes

- **Use `insphere_distance`** to understand geometric intuition
- Explicit circumcenter calculation makes algorithm transparent
- Excellent for debugging and algorithm validation
- Useful for educational materials despite slower performance

### Performance Summary

Based on current benchmarks:

- `insphere_lifted`: 929 ns (fastest)
- `insphere`: 1.1 µs (balanced)
- `insphere_distance`: 1.7 µs (transparent)

## Conclusion

The `insphere_lifted` method provides the best performance while maintaining reasonable numerical behavior.
For most applications requiring high-performance circumsphere containment tests, it should be the preferred choice.

The standard `insphere` method remains the most numerically stable option when correctness is prioritized over performance.

## Historical Version Comparison

*Based on archived performance measurements from previous releases:*

### v0.3.0 → v0.3.1 Performance Improvements

| Test Case | Method | v0.3.0 | v0.3.1 | Improvement |
|-----------|--------|--------|--------|-------------|
| Basic 3D | insphere | 808 ns | 805 ns | +0.4% |
| Basic 3D | insphere_distance | 1,505 ns | 1,463 ns | +2.8% |
| Basic 3D | insphere_lifted | 646 ns | 637 ns | +1.4% |
| Random 1000 queries | insphere | 822 µs | 811 µs | +1.3% |
| Random 1000 queries | insphere_distance | 1,535 µs | 1,494 µs | +2.7% |
| Random 1000 queries | insphere_lifted | 661 µs | 650 µs | +1.7% |
| 2D | insphere_lifted | 442 ns | 440 ns | +0.5% |
| 4D | insphere_lifted | 962 ns | 955 ns | +0.7% |

**Key Improvements**: Version 0.3.1 showed consistent performance gains across all methods,
with `insphere_distance` seeing the largest improvement (+2.8%). The changes implemented
improved numerical stability using `hypot` and `squared_norm` functions while providing
measurable performance gains.

## Implementation Notes

### Performance Advantages of `insphere_lifted`

1. More efficient matrix formulation using relative coordinates
2. Avoids redundant circumcenter calculations
3. Optimized determinant computation

### Method Disagreements

The disagreements between methods are expected due to:

1. Different numerical approaches and tolerances
2. Floating-point precision differences in multi-step calculations
3. Varying sensitivity to degenerate cases

## Benchmark Structure

The `circumsphere_containment.rs` benchmark includes:

- **Random queries**: Batch processing performance with 1000 random test points
- **Dimensional tests**: Performance across 2D, 3D, 4D, and 5D simplices
- **Edge cases**: Boundary vertices and far-away points
- **Numerical consistency**: Agreement analysis between all methods

## Performance Data Updates

This file is automatically generated from benchmark results. To update:

```bash
# Generate performance summary with current data
uv run benchmark-utils generate-summary

# Run fresh benchmarks and generate summary (includes numerical accuracy)
uv run benchmark-utils generate-summary --run-benchmarks

# Generate baseline results for regression testing
uv run benchmark-utils generate-baseline
```

### Customization

For manual updates or custom analysis, modify the `PerformanceSummaryGenerator`
class in `scripts/benchmark_utils.py`. This provides enhanced control over
dynamic vs static content organization and supports parsing numerical accuracy
data from live benchmark runs.
