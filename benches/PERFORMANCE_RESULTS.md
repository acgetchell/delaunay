# Delaunay Library Performance Results

This file contains performance benchmarks and analysis for the delaunay library.
The results are automatically generated and updated by the benchmark infrastructure.

**Last Updated**: 2026-02-20 21:30:13 UTC
**Generated By**: benchmark_utils.py
**Git Commit**: dc7a3684452f63e9fdf7bcf9e456ffc4ea41cc56
**Hardware**: Apple M4 Max (16 cores)
**Memory**: 64.0 GB
**OS**: macOS
**Rust**: rustc 1.93.0 (254b59607 2026-01-19)

## Performance Results Summary

### Circumsphere Performance Results

#### Version 0.7.1 Results (2026-02-20)

#### Single Query Performance (2D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 2D | 51 ns | 32 ns | 40 ns | **insphere_distance** |
| Boundary vertex | 3 ns | 32 ns | 40 ns | **insphere** |
| Far vertex | 41 ns | 32 ns | 45 ns | **insphere_distance** |

#### Single Query Performance (3D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 3D | 127 ns | 55 ns | 75 ns | **insphere_distance** |
| Boundary vertex | 3 ns | 55 ns | 74 ns | **insphere** |
| Far vertex | 93 ns | 55 ns | 79 ns | **insphere_distance** |

#### Single Query Performance (4D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 4D | 188 ns | 76 ns | 147 ns | **insphere_distance** |
| Boundary vertex | 3 ns | 75 ns | 147 ns | **insphere** |
| Far vertex | 146 ns | 76 ns | 159 ns | **insphere_distance** |

#### Single Query Performance (5D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 5D | 264 ns | 90 ns | 205 ns | **insphere_distance** |
| Boundary vertex | 4 ns | 90 ns | 203 ns | **insphere** |
| Far vertex | 196 ns | 90 ns | 212 ns | **insphere_distance** |

## Key Findings

### Performance Ranking

1. **insphere_distance** - (best in 2D, 3D, 4D, 5D) - Best average performance
2. **insphere_lifted** - ~1.9x slower than fastest on average
3. **insphere** - ~2.2x slower than fastest on average

### Numerical Accuracy Analysis

Based on random test cases:

- **insphere vs insphere_distance**: 100.0% agreement
- **insphere vs insphere_lifted**: 100.0% agreement (different algorithms)
- **insphere_distance vs insphere_lifted**: 100.0% agreement
- **All three methods agree**: 100.0% (expected due to different numerical approaches)

## Recommendations

### Method Selection Guide

**All three methods are mathematically correct** (they produce valid insphere test results).
Choose based on your specific requirements:

#### Performance Optimization by Dimension

- **`insphere_distance`**: (best in 2D, 3D, 4D, 5D) - Best average performance

#### General Recommendations

**For maximum performance**: Choose the method that performs best in your target dimension (see above)

**For general-purpose use**: `insphere` provides consistent performance across all dimensions
and uses the standard determinant-based approach with well-understood numerical properties

**For algorithm transparency**: `insphere_distance` explicitly calculates the circumcenter,
making it excellent for educational purposes, debugging, and algorithm validation

#### Performance Comparison

Average performance across all non-boundary test cases:

- `insphere_distance`: 63 ns (best in 2D, 3D, 4D, 5D)
- `insphere_lifted`: 120 ns (second fastest)
- `insphere`: 138 ns (third fastest)

## Conclusion

All three methods are mathematically correct and produce valid results. Performance characteristics vary by dimension:

- `insphere_distance` (best in 2D, 3D, 4D, 5D) - Best average performance

For general-purpose applications, choose based on your primary use case:

- **Performance-critical**: Use the method that performs best in your target dimension
- **Numerical stability**: Use `insphere` for its proven mathematical properties
- **Educational/debugging**: Use `insphere_distance` for its transparent algorithm

## Historical Version Comparison

*Based on archived performance measurements from previous releases:*

### v0.3.0 → v0.3.1 Performance Improvements

| Test Case | Method | v0.3.0 | v0.3.1 | Improvement |
|-----------|--------|--------|--------|-------------|
| Basic 3D | insphere | 808 ns | 805 ns | +0.4% |
| Basic 3D | insphere_distance | 1,505 ns | 1,463 ns | +2.8% |
| Basic 3D | insphere_lifted | 646 ns | 637 ns | +1.4% |
| Random 1000 queries | insphere | 822 µs | 811 µs | +1.3% |
| Random 1000 queries | insphere_distance | 1,535 µs | 1,494 µs | +2.7% |
| Random 1000 queries | insphere_lifted | 661 µs | 650 µs | +1.7% |
| 2D | insphere_lifted | 442 ns | 440 ns | +0.5% |
| 4D | insphere_lifted | 962 ns | 955 ns | +0.7% |

**Key Improvements**: Version 0.3.1 showed consistent performance gains across all methods,
with `insphere_distance` seeing the largest improvement (+2.8%). The changes implemented
improved numerical stability using `hypot` and `squared_norm` functions while providing
measurable performance gains.

## Implementation Notes

### Performance Advantages of `insphere_lifted`

1. More efficient matrix formulation using relative coordinates
2. Avoids redundant circumcenter calculations
3. Optimized determinant computation

### Method Disagreements

The disagreements between methods are expected due to:

1. Different numerical approaches and tolerances
2. Floating-point precision differences in multi-step calculations
3. Varying sensitivity to degenerate cases

## Benchmark Structure

The `circumsphere_containment.rs` benchmark includes:

- **Random queries**: Batch processing performance with 1000 random test points
- **Dimensional tests**: Performance across 2D, 3D, 4D, and 5D simplices
- **Edge cases**: Boundary vertices and far-away points
- **Numerical consistency**: Agreement analysis between all methods

## Performance Data Updates

This file is automatically generated from benchmark results. To update:

```bash
# Generate performance summary with current data
uv run benchmark-utils generate-summary

# Run fresh benchmarks and generate summary (includes numerical accuracy)
uv run benchmark-utils generate-summary --run-benchmarks

# Generate baseline results for regression testing
uv run benchmark-utils generate-baseline
```

### Customization

For manual updates or custom analysis, modify the `PerformanceSummaryGenerator`
class in `scripts/benchmark_utils.py`. This provides enhanced control over
dynamic vs static content organization and supports parsing numerical accuracy
data from live benchmark runs.
