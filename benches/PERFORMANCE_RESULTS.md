# Delaunay Library Performance Results

This file contains performance benchmarks and analysis for the delaunay library.
The results are automatically generated and updated by the benchmark infrastructure.

**Last Updated**: 2025-09-09 05:04:51 UTC
**Generated By**: performance_summary_utils.py
**Git Commit**: 56f322d875080eb2dcaef753184d6a5898256251
**Hardware**: Apple M4 Max (16 cores)
**Memory**: 64.0 GB
**OS**: macOS
**Rust**: rustc 1.89.0 (29483883e 2025-08-04)

## Performance Results Summary

### Version unknown Results (2025-09-09)

#### Single Query Performance (2D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 2D | 560 ns | 644 ns | 448 ns | **insphere_lifted** |
| Boundary vertex | 570 ns | 644 ns | 451 ns | **insphere_lifted** |
| Far vertex | 570 ns | 641 ns | 449 ns | **insphere_lifted** |

#### Single Query Performance (3D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 3D | 846 ns | 1.5 µs | 657 ns | **insphere_lifted** |
| Boundary vertex | 854 ns | 1.5 µs | 664 ns | **insphere_lifted** |
| Far vertex | 849 ns | 1.5 µs | 660 ns | **insphere_lifted** |

#### Single Query Performance (4D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 4D | 1.2 µs | 1.9 µs | 979 ns | **insphere_lifted** |
| Boundary vertex | 1.3 µs | 1.9 µs | 987 ns | **insphere_lifted** |
| Far vertex | 1.3 µs | 1.9 µs | 975 ns | **insphere_lifted** |

#### Single Query Performance (5D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 5D | 1.8 µs | 3.0 µs | 1.5 µs | **insphere_lifted** |
| Boundary vertex | 1.8 µs | 3.1 µs | 1.5 µs | **insphere_lifted** |
| Far vertex | 1.8 µs | 3.0 µs | 1.5 µs | **insphere_lifted** |

### Historical Version Comparison

*Based on archived performance measurements from previous releases:*

#### v0.3.0 → v0.3.1 Performance Improvements

| Test Case | Method | v0.3.0 | v0.3.1 | Improvement |
|-----------|--------|--------|--------|-------------|
| Basic 3D | insphere | 808 ns | 805 ns | +0.4% |
| Basic 3D | insphere_distance | 1,505 ns | 1,463 ns | +2.8% |
| Basic 3D | insphere_lifted | 646 ns | 637 ns | +1.4% |
| Random 1000 queries | insphere | 822 µs | 811 µs | +1.3% |
| Random 1000 queries | insphere_distance | 1,535 µs | 1,494 µs | +2.7% |
| Random 1000 queries | insphere_lifted | 661 µs | 650 µs | +1.7% |
| 2D | insphere_lifted | 442 ns | 440 ns | +0.5% |
| 4D | insphere_lifted | 962 ns | 955 ns | +0.7% |

**Key Improvements**: Version 0.3.1 showed consistent performance gains across all methods,
with `insphere_distance` seeing the largest improvement (+2.8%). The changes implemented
improved numerical stability using `hypot` and `squared_norm` functions while providing
measurable performance gains.

## Key Findings

### Performance Ranking

1. **insphere_lifted** - (fastest) - Consistently best performance across all tests
2. **insphere** - (middle) - ~1.3x slower than fastest, but good performance
3. **insphere_distance** - (slowest) - ~2.0x slower due to explicit circumcenter calculation

### Numerical Accuracy Analysis

Based on 1000 random test cases:

- **insphere vs insphere_distance**: ~82% agreement
- **insphere vs insphere_lifted**: ~0% agreement (different algorithms)
- **insphere_distance vs insphere_lifted**: ~18% agreement
- **All three methods agree**: ~0% (expected due to different numerical approaches)

## Recommendations

### For Performance-Critical Applications

- **Use `insphere_lifted`** for maximum performance
- ~2.0x faster than `insphere_distance`
- Best choice for batch processing and high-frequency queries
- Recommended for applications requiring millions of containment tests

### For Numerical Stability

- **Use `insphere`** for most reliable results
- Standard determinant-based approach with proven mathematical properties
- Good balance of performance and numerical stability
- Recommended for applications where correctness is paramount

### For Educational/Research Purposes

- **Use `insphere_distance`** to understand geometric intuition
- Explicit circumcenter calculation makes algorithm transparent
- Excellent for debugging and algorithm validation
- Useful for educational materials despite slower performance

### Performance Summary

Based on current benchmarks:

- `insphere_lifted`: 897 ns (fastest)
- `insphere`: 1.1 µs (balanced)
- `insphere_distance`: 1.8 µs (transparent)

## Implementation Notes

### Performance Advantages of `insphere_lifted`

1. More efficient matrix formulation using relative coordinates
2. Avoids redundant circumcenter calculations
3. Optimized determinant computation

### Method Disagreements

The disagreements between methods are expected due to:

1. Different numerical approaches and tolerances
2. Floating-point precision differences in multi-step calculations
3. Varying sensitivity to degenerate cases

## Benchmark Structure

The `circumsphere_containment.rs` benchmark includes:

- **Random queries**: Batch processing performance with 1000 random test points
- **Dimensional tests**: Performance across 2D, 3D, 4D, and 5D simplices
- **Edge cases**: Boundary vertices and far-away points
- **Numerical consistency**: Agreement analysis between all methods

## Conclusion

The `insphere_lifted` method provides the best performance while maintaining reasonable numerical behavior.
For most applications requiring high-performance circumsphere containment tests, it should be the preferred choice.

The standard `insphere` method remains the most numerically stable option when correctness is prioritized over performance.

## Performance Data Updates

This file is automatically generated from benchmark results. To update:

```bash
# Generate fresh performance summary
uv run performance-summary-utils generate

# Run benchmarks and generate summary
uv run performance-summary-utils generate --run-benchmarks

# Generate baseline results (separate utility)
uv run benchmark-utils generate-baseline
```

For manual updates or custom analysis, modify the `PerformanceSummaryGenerator`
class in `scripts/performance_summary_utils.py`.
