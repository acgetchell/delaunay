# Delaunay Library Performance Results

This file contains performance benchmarks and analysis for the delaunay library.
The results are automatically generated and updated by the benchmark infrastructure.

**Last Updated**: 2025-11-25 05:03:43 UTC
**Generated By**: benchmark_utils.py
**Git Commit**: 37ccaecab39e7951ee9fbc3da7f7dd1556bdbf95
**Hardware**: Apple M4 Max (16 cores)
**Memory**: 64.0 GB
**OS**: macOS
**Rust**: rustc 1.91.0 (f8297e351 2025-10-28)

## Performance Results Summary

### Circumsphere Performance Results

#### Version 0.5.4 Results (2025-11-24)

#### Single Query Performance (2D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 2D | 108 ns | 78 ns | 41 ns | **insphere_lifted** |
| Boundary vertex | 3 ns | 79 ns | 42 ns | **insphere** |
| Far vertex | 102 ns | 78 ns | 43 ns | **insphere_lifted** |

#### Single Query Performance (3D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 3D | 208 ns | 89 ns | 166 ns | **insphere_distance** |
| Boundary vertex | 4 ns | 87 ns | 164 ns | **insphere** |
| Far vertex | 204 ns | 86 ns | 163 ns | **insphere_distance** |

#### Single Query Performance (4D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 4D | 288 ns | 177 ns | 224 ns | **insphere_distance** |
| Boundary vertex | 4 ns | 178 ns | 225 ns | **insphere** |
| Far vertex | 280 ns | 177 ns | 228 ns | **insphere_distance** |

#### Single Query Performance (5D)

| Test Case | insphere | insphere_distance | insphere_lifted | Winner |
|-----------|----------|------------------|-----------------|---------|
| Basic 5D | 364 ns | 361 ns | 290 ns | **insphere_lifted** |
| Boundary vertex | 4 ns | 361 ns | 292 ns | **insphere** |
| Far vertex | 360 ns | 366 ns | 297 ns | **insphere_lifted** |

## Key Findings

### Performance Ranking

1. **insphere** - (fastest average) - Best performance on boundary vertex cases, competitive on others
2. **insphere_distance** - (middle) - ~1.1x slower than fastest, best on 3D/4D basic cases
3. **insphere_lifted** - (slowest) - ~1.1x slower than fastest, best on 2D/5D basic cases

### Numerical Accuracy Analysis

Based on random test cases:

- **insphere vs insphere_distance**: 100.0% agreement
- **insphere vs insphere_lifted**: 100.0% agreement (different algorithms)
- **insphere_distance vs insphere_lifted**: 100.0% agreement
- **All three methods agree**: 100.0% (expected due to different numerical approaches)

## Recommendations

### For Performance-Critical Applications

- **Use `insphere`** for maximum performance
- Best choice for batch processing and high-frequency queries
- Recommended for applications requiring millions of containment tests

### For Numerical Stability

- **Use `insphere`** for most reliable results
- Standard determinant-based approach with proven mathematical properties
- Good balance of performance and numerical stability
- Recommended for applications where correctness is paramount

### For Educational/Research Purposes

- **Use `insphere_distance`** to understand geometric intuition
- Explicit circumcenter calculation makes algorithm transparent
- Excellent for debugging and algorithm validation
- Useful for educational materials despite slower performance

### Performance Summary

Based on current benchmarks:

- `insphere`: 161 ns (fastest)
- `insphere_distance`: 177 ns (balanced)
- `insphere_lifted`: 181 ns (transparent)

## Conclusion

The `insphere` method provides the best performance while maintaining reasonable numerical behavior.
For most applications requiring high-performance circumsphere containment tests, it should be the preferred choice.

The standard `insphere` method remains the most numerically stable option when correctness is prioritized over performance.

## Historical Version Comparison

*Based on archived performance measurements from previous releases:*

### v0.3.0 → v0.3.1 Performance Improvements

| Test Case | Method | v0.3.0 | v0.3.1 | Improvement |
|-----------|--------|--------|--------|-------------|
| Basic 3D | insphere | 808 ns | 805 ns | +0.4% |
| Basic 3D | insphere_distance | 1,505 ns | 1,463 ns | +2.8% |
| Basic 3D | insphere_lifted | 646 ns | 637 ns | +1.4% |
| Random 1000 queries | insphere | 822 µs | 811 µs | +1.3% |
| Random 1000 queries | insphere_distance | 1,535 µs | 1,494 µs | +2.7% |
| Random 1000 queries | insphere_lifted | 661 µs | 650 µs | +1.7% |
| 2D | insphere_lifted | 442 ns | 440 ns | +0.5% |
| 4D | insphere_lifted | 962 ns | 955 ns | +0.7% |

**Key Improvements**: Version 0.3.1 showed consistent performance gains across all methods,
with `insphere_distance` seeing the largest improvement (+2.8%). The changes implemented
improved numerical stability using `hypot` and `squared_norm` functions while providing
measurable performance gains.

## Implementation Notes

### Performance Advantages of `insphere_lifted`

1. More efficient matrix formulation using relative coordinates
2. Avoids redundant circumcenter calculations
3. Optimized determinant computation

### Method Disagreements

The disagreements between methods are expected due to:

1. Different numerical approaches and tolerances
2. Floating-point precision differences in multi-step calculations
3. Varying sensitivity to degenerate cases

## Benchmark Structure

The `circumsphere_containment.rs` benchmark includes:

- **Random queries**: Batch processing performance with 1000 random test points
- **Dimensional tests**: Performance across 2D, 3D, 4D, and 5D simplices
- **Edge cases**: Boundary vertices and far-away points
- **Numerical consistency**: Agreement analysis between all methods

## Performance Data Updates

This file is automatically generated from benchmark results. To update:

```bash
# Generate performance summary with current data
uv run benchmark-utils generate-summary

# Run fresh benchmarks and generate summary (includes numerical accuracy)
uv run benchmark-utils generate-summary --run-benchmarks

# Generate baseline results for regression testing
uv run benchmark-utils generate-baseline
```

### Customization

For manual updates or custom analysis, modify the `PerformanceSummaryGenerator`
class in `scripts/benchmark_utils.py`. This provides enhanced control over
dynamic vs static content organization and supports parsing numerical accuracy
data from live benchmark runs.
